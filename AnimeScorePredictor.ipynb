{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Model to Predict Anime Score\n",
    "The purpose of this project is to practice skills in data analysis and machine learning.\n",
    "To achieve this, I attempt to predict what a anime's MyAnimeList score will be given certain parameters.\n",
    "\n",
    "Scenario: As an indie company, can we predict the score our project will get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'E:\\AnimeAnalysis\\AnimeList.csv'\n",
    "main_df = pd.read_csv(csv_path)\n",
    "df_copy = main_df\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['title_english', 'title_japanese',\n",
    "       'title_synonyms', 'image_url', 'status',\n",
    "       'airing', 'aired_string', 'aired','duration', 'rating',  'background',\n",
    "       'premiered', 'broadcast', 'related',  'opening_theme', 'ending_theme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneeded columns\n",
    "df_cleanned = df_copy.drop(columns = drop_cols, inplace=False, axis=1)\n",
    "df_cleanned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanned = df_cleanned[df_cleanned.score > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching data types and filling in nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in null rank column with zeros\n",
    "df_cleanned['rank'] = df_cleanned['rank'].fillna(0)\n",
    "#Fill in rest of null values with 'unknown'\n",
    "df_cleanned.fillna('unknown', inplace=True)\n",
    "df_cleanned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change col:rank to int \n",
    "df_cleanned['rank'] = df_cleanned['rank'].astype('int64')\n",
    "df_cleanned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change text to uppercase\n",
    "df_cleanned['type'] = df_cleanned['type'].str.upper()\n",
    "df_cleanned['source'] = df_cleanned['source'].str.upper()\n",
    "df_cleanned['producer'] = df_cleanned['producer'].str.upper()\n",
    "df_cleanned['licensor'] = df_cleanned['licensor'].str.upper()\n",
    "df_cleanned['studio'] = df_cleanned['studio'].str.upper()\n",
    "df_cleanned['genre'] = df_cleanned['genre'].str.upper()\n",
    "\n",
    "df_cleanned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to csv for Tableau visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleanned.to_csv('AnimeListClean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot variables against each for possible correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleaned.corr()\n",
    "#sns.pairplot(df_cleanned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change type and source into numbers (encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dummy = pd.get_dummies(df_cleanned['type'], prefix='t')\n",
    "src_dummy = pd.get_dummies(df_cleanned['source'], prefix='src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dummy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanned = pd.concat([df_cleanned, type_dummy, src_dummy], axis=1)\n",
    "df_cleanned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select feature(X) and dependant variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleanned[['score']]\n",
    "X = df_cleanned.drop(['anime_id','title','type','source','episodes','score','scored_by','rank','popularity','members','favorites','producer','licensor','studio','genre'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.info())\n",
    "print(X.info())\n",
    "df_cleanned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept of the linear equation:\", lr_model.intercept_) \n",
    "print(\"\\nCOefficients of the equation are:\", lr_model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = pd.DataFrame(lr_model.predict(X_test), columns=['Predicted Score'])\n",
    "yhat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualScore = y_test.reset_index(drop=True) # Drop the index so that we can concat it, to create new dataframe\n",
    "df_actual_vs_predicted = pd.concat([actualScore,yhat],axis =1)\n",
    "df_actual_vs_predicted.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring the performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define scoring function\n",
    "def score_model(y_test, yhat):\n",
    "    #closer to zero is better\n",
    "    print('MAE: ', metrics.mean_absolute_error(y_test, yhat))\n",
    "\n",
    "    # Closer to zero is better\n",
    "    print('MSE: ', metrics.mean_squared_error(y_test, yhat))\n",
    "\n",
    "    # Closer to zero is better\n",
    "    print('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, yhat)))\n",
    "\n",
    "    # Closer to one is better\n",
    "    print('R^2: ', metrics.r2_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the true value vs the predicted value\n",
    "def plot_test(xtest, yframe):\n",
    "    for columns in X_test:\n",
    "        sns.scatterplot(x=X_test[columns], y=yframe['score'] )\n",
    "        sns.scatterplot(x=X_test[columns]+.05, y=yframe['Predicted Score'] ) # +0.5 Shifts the predicted value to the right for better visual\n",
    "        plt.legend(['actual','predicted'], loc=\"lower center\")\n",
    "        plt.title(columns)\n",
    "        plt.show()\n",
    "\n",
    "#plot_test(X_test, df_actual_vs_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_train, Xp_test, yp_train, yp_test = train_test_split(X_poly,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model = LinearRegression().fit(Xp_train, yp_train)\n",
    "poly_model.score(Xp_train, yp_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_hat = pd.DataFrame(poly_model.predict(Xp_test), columns=['Predicted Score'])\n",
    "yp_hat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_actualScore = yp_test.reset_index(drop=True) # Drop the index so that we can concat it, to create new dataframe\n",
    "dfp_actual_vs_predicted = pd.concat([p_actualScore,yp_hat],axis =1)\n",
    "dfp_actual_vs_predicted.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(yp_test, yp_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_test(Xp_test, dfp_actual_vs_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use Genre, Producer, etc to predict the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a set of genre, this removes duplicates\n",
    "def get_vals(colname, df=df_cleanned):\n",
    "    elements = set()\n",
    "    for name, value in df[colname].iteritems():\n",
    "        value = value.split(', ')\n",
    "        elements.update(value)\n",
    "    return elements\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = get_vals('genre')\n",
    "#print(genres)\n",
    "producers, licensors, studios = get_vals('producer'), get_vals('licensor'), get_vals('studio')\t\n",
    "#print(producers)\n",
    "#print(licensors)\n",
    "#print(studios)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add genres as columns\n",
    "def add_cols(list, prefix, df=df_cleanned):\n",
    "    for i in list:\n",
    "        df[prefix + '_' + i] = 0 #fills column cells with zeros\n",
    "        df[prefix + '_' + i] = df[prefix + '_' + i].astype('uint8')\n",
    "    print(\"Columns added.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cols(genres,'g')\n",
    "df_cleanned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to set value to 1 if anime is of the genre\n",
    "def set_val(column, prefix, df=df_cleanned):\n",
    "    for i, value in df[column].iteritems():\n",
    "        value = value.split(', ')\n",
    "        for h in value:\n",
    "            df.loc[i, prefix + '_' + h] = 1\n",
    "    print(\"Values set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_val('genre','g')\n",
    "df_cleanned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleanned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_genre = df_cleanned[['score']]\n",
    "X_genre = df_cleanned.drop(['anime_id', 'title', 'type', 'source', 'episodes', 'score', 'scored_by',\n",
    "       'rank', 'popularity', 'members', 'favorites', 'producer', 'licensor',\n",
    "       'studio', 'genre', 't_MOVIE', 't_MUSIC', 't_ONA', 't_OVA', 't_SPECIAL',\n",
    "       't_TV', 't_UNKNOWN', 'src_4-KOMA MANGA', 'src_BOOK', 'src_CARD GAME',\n",
    "       'src_DIGITAL MANGA', 'src_GAME', 'src_LIGHT NOVEL', 'src_MANGA',\n",
    "       'src_MUSIC', 'src_NOVEL', 'src_ORIGINAL', 'src_OTHER',\n",
    "       'src_PICTURE BOOK', 'src_RADIO', 'src_UNKNOWN', 'src_VISUAL NOVEL',\n",
    "       'src_WEB MANGA'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_genre,y_genre, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train lr model\n",
    "lr_model2 = LinearRegression().fit(Xg_train,yg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model2.score(Xg_train,yg_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept of the linear equation:\", lr_model2.intercept_) \n",
    "print(\"\\nCOefficients of the equation are:\", lr_model2.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yg_hat = pd.DataFrame(lr_model2.predict(Xg_test), columns=['Predicted Score'])\n",
    "yg_hat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actualScore2 = yg_test.reset_index(drop=True) # Drop the index so that we can concat it, to create new dataframe\n",
    "df_actual_vs_predicted2 = pd.concat([actualScore2,yg_hat],axis =1)\n",
    "df_actual_vs_predicted2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(yg_test,yg_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cols(producers,'p')\n",
    "add_cols(licensors,'l')\n",
    "add_cols(studios,'st')\n",
    "set_val('producer','p')\n",
    "set_val('licensor','l')\n",
    "set_val('studio','st')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = df_cleanned['score']\n",
    "X_all = df_cleanned.drop(['anime_id', 'title', 'type', 'source', 'episodes', 'score', 'scored_by',\n",
    "       'rank', 'popularity', 'members', 'favorites', 'producer', 'licensor',\n",
    "       'studio', 'genre'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting top 10 best features by applying SelectKBest class\n",
    "bestfeatures = SelectKBest(score_func=mutual_info_regression, k=10)\n",
    "fit = bestfeatures.fit(X_all,y_all)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_all.columns)\n",
    " \n",
    "#concat two dataframes\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topFeatures = featureScores.nlargest(50,'Score')  #printing 10 best features\n",
    "topFeatures.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topFeatures['Specs'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = df_cleanned[['score']]\n",
    "X_final = df_cleanned[topFeatures['Specs'].values]\n",
    "X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf_train, Xf_test, yf_train, yf_test = train_test_split(X_final, y_final, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model3 = LinearRegression().fit(Xf_train, yf_train)\n",
    "yf_hat = pd.DataFrame(lr_model3.predict(Xf_test), columns=['Predicted Score'])\n",
    "score_model(yf_test, yf_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualScore3 = yf_test.reset_index(drop=True) # Drop the index so that we can concat it, to create new dataframe\n",
    "df_actual_vs_predicted3 = pd.concat([actualScore3,yf_hat],axis =1)\n",
    "df_actual_vs_predicted3.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Given the available parameters, we cannot accurately predict the score of an Anime with enough confidence that it is not by random chance.\n",
    "\n",
    "__Reasons:__\n",
    "- Score given to an anime by a watcher is very subjective.\n",
    "- Data does not capture the necessary features to predict score with high level of confidence.\n",
    "\n",
    "__Other Observations:__\n",
    "- Going in, I thought genre and studio would be a strong feature to predict the score."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5aa0460419c6f3c9ea2b65ef75561dca8defbd48bed57ba42e16c8f184ed6c0e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "5aa0460419c6f3c9ea2b65ef75561dca8defbd48bed57ba42e16c8f184ed6c0e"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
